{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "587f1013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Model vocab size is 250027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "from typing import List\n",
    "from utils import remove_punct, tokenize\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    MBartTokenizer,\n",
    "    MBartForConditionalGeneration,\n",
    "    BartTokenizer,\n",
    "    BartForConditionalGeneration,\n",
    ")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "SEQ_TO_SEQ_MODEL = \"mbart\"\n",
    "LANGUAGE = \"fi\"\n",
    "DATASET = \"wikisource\"\n",
    "SPECIAL_TOKENS = []\n",
    "CHECKPOINT = \"\"\n",
    "\n",
    "\n",
    "language_codes = {\n",
    "    \"en\": \"en_XX\",\n",
    "    \"fi\": \"fi_FI\",\n",
    "}\n",
    "\n",
    "########################################################################################################################\n",
    "# load model and tokenizer\n",
    "########################################################################################################################\n",
    "\n",
    "MODEL_DIR = os.path.join(\n",
    "    os.environ[\"MODELDIR_UNI\"],\n",
    "    \"finnishPoetryGeneration\",\n",
    "    \"ukko2\",\n",
    "    \"{}-{}-{}\".format(DATASET, LANGUAGE, SEQ_TO_SEQ_MODEL),\n",
    ")\n",
    "if CHECKPOINT:\n",
    "    MODEL_DIR = os.path.join(MODEL_DIR, CHECKPOINT)\n",
    "\n",
    "if SEQ_TO_SEQ_MODEL == \"mbart\":\n",
    "    MODEL = \"facebook/mbart-large-cc25\"\n",
    "\n",
    "    tokenizer = MBartTokenizer.from_pretrained(\n",
    "        MODEL,\n",
    "        src_lang=language_codes[LANGUAGE],\n",
    "        tgt_lang=language_codes[LANGUAGE],\n",
    "        additional_special_tokens=SPECIAL_TOKENS,\n",
    "    )\n",
    "    model = MBartForConditionalGeneration.from_pretrained(MODEL)\n",
    "    model.config.decoder_start_token_id = tokenizer.lang_code_to_id[\n",
    "        language_codes[LANGUAGE]\n",
    "    ]\n",
    "elif SEQ_TO_SEQ_MODEL == \"bart\":\n",
    "    MODEL = \"facebook/bart-large\"\n",
    "\n",
    "    tokenizer = BartTokenizer.from_pretrained(\n",
    "        MODEL, additional_special_tokens=SPECIAL_TOKENS\n",
    "    )\n",
    "    model = BartForConditionalGeneration.from_pretrained(MODEL)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "logging.info(\"Model vocab size is {}\".format(model.config.vocab_size))\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        os.path.join(MODEL_DIR, \"pytorch_model.bin\"), map_location=torch.device(\"cpu\")\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# Helper functions\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "def get_ngrams(text: str, n: int) -> List[str]:\n",
    "    text_chars = \"\".join(c for c in text if c.isalpha())\n",
    "    return [text_chars[i : i + n] for i in range(len(text_chars) - n)]\n",
    "\n",
    "\n",
    "def ngram_similarity(string1: str, string2: str, n: int) -> float:\n",
    "\n",
    "    ngrams_1 = set(get_ngrams(string1, n))\n",
    "    ngrams_2 = set(get_ngrams(string2, n))\n",
    "\n",
    "    common_ngrams = ngrams_1.intersection(ngrams_2)\n",
    "    all_ngrams = ngrams_1.union(ngrams_2)\n",
    "\n",
    "    return len(common_ngrams) / len(all_ngrams)\n",
    "\n",
    "\n",
    "def token_similarity(string1: str, string2: str) -> float:\n",
    "\n",
    "    tokens_1 = set([t.lower() for t in tokenize(remove_punct(string1))])\n",
    "    tokens_2 = set([t.lower() for t in tokenize(remove_punct(string2))])\n",
    "\n",
    "    common_tokens = tokens_1.intersection(tokens_2)\n",
    "    all_tokens = tokens_1.union(tokens_2)\n",
    "\n",
    "    return len(common_tokens) / len(all_tokens)\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# Generate text\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "def get_next_line_candidates(\n",
    "    input_line: str, keywords: List[str] = None, separator: str = \">>>SEP<<<\"\n",
    ") -> List[str]:\n",
    "    if keywords:\n",
    "        source = (\n",
    "            \" \".join(random.sample(keywords, max(len(keywords) - 1, 1)))\n",
    "            + \" \"\n",
    "            + separator\n",
    "            + \" \"\n",
    "            + input_line\n",
    "        )\n",
    "    else:\n",
    "        source = input_line\n",
    "    logging.debug(source)\n",
    "    encoded = tokenizer.encode(\n",
    "        source, padding=\"max_length\", max_length=32, truncation=True\n",
    "    )\n",
    "    encoded = torch.tensor(encoded).unsqueeze(0)\n",
    "\n",
    "    sample_outputs = model.generate(\n",
    "        encoded,\n",
    "        do_sample=True,\n",
    "        max_length=16,\n",
    "        num_beams=5,\n",
    "        # repetition_penalty=5.0,\n",
    "        early_stopping=True,\n",
    "        num_return_sequences=5,\n",
    "    )\n",
    "\n",
    "    candidates = [\n",
    "        tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "        for sample_output in sample_outputs\n",
    "    ]\n",
    "    logging.info(\"Generated candidates {}\".format(candidates))\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def get_next_line(\n",
    "    input_line: str, keywords: List[str] = None, separator: str = \">>>SEP<<<\"\n",
    ") -> str:\n",
    "\n",
    "    candidates = get_next_line_candidates(input_line, keywords, separator)\n",
    "\n",
    "    candidates = [\n",
    "        candidate for candidate in candidates if remove_punct(candidate)\n",
    "    ]  # make sure that lines containing only punctuation are excluded\n",
    "\n",
    "    if not candidates:\n",
    "        return input_line\n",
    "\n",
    "    # compute character n-gram similarity and token similarity with the input line for every candidate\n",
    "    scored_candidates = [\n",
    "        (\n",
    "            candidate,\n",
    "            token_similarity(input_line, candidate),\n",
    "            ngram_similarity(input_line, candidate, 3),\n",
    "        )\n",
    "        for candidate in candidates\n",
    "    ]\n",
    "\n",
    "    candidates = [candidate for candidate in scored_candidates if candidate[1] < 0.4]\n",
    "\n",
    "    if not candidates:\n",
    "        logging.debug(\n",
    "            \"No candidates with token similarity lower than threshold value. Returning candidate with lower \"\n",
    "            \"similarity\"\n",
    "        )\n",
    "        return sorted(scored_candidates, key=lambda x: x[1])[0][0]\n",
    "\n",
    "    return sorted(candidates, key=lambda x: x[2])[0][0]\n",
    "\n",
    "\n",
    "def iterative_generation(\n",
    "    input_line: str,\n",
    "    lines: int,\n",
    "    keywords: List[str] = None,\n",
    "    separator: str = \">>>SEP<<<\",\n",
    ") -> str:\n",
    "\n",
    "    out = input_line\n",
    "    last_line = input_line\n",
    "    counter = 0\n",
    "    stanza_length = 0\n",
    "    while True:\n",
    "        next_line = get_next_line(last_line, keywords, separator)\n",
    "        if last_line[-1] == \"!\" and next_line[-1] == \"!\":\n",
    "            next_line = next_line[:-1] + \".\"\n",
    "        elif last_line[-1] == \".\" and next_line[-1] == \".\":\n",
    "            next_line = next_line[:-1] + \",\"\n",
    "        logging.info(\"Generated poetry line '{}'\".format(next_line))\n",
    "        counter += 1\n",
    "        out += \"\\n\"\n",
    "        out += next_line\n",
    "        stanza_length += 1\n",
    "        if next_line.strip()[-1] in [\".\", \"!\", \"?\"]:\n",
    "            if counter >= lines:\n",
    "                break\n",
    "            elif stanza_length > 1:\n",
    "                out += \"\\n\"\n",
    "                stanza_length = 0\n",
    "        last_line = next_line\n",
    "\n",
    "    return out\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fc9c64",
   "metadata": {},
   "source": [
    "# Finnish Poetry Generation\n",
    "\n",
    "Current implementation - in short:\n",
    "\n",
    " * one generative model takes keywords as input and generates the first poem line;\n",
    " * another generative model takes the 1st line as input, and generate the next verse. This model is used iteratively;\n",
    " * each generative model returns multiple outputs, users can select the best candidate verse. For protityping, we select the candidate automatically by implementing a simple heuristic;\n",
    " * iterative generation is stopped when the generated verse ends with a full stop, and the total number of generated lines is larger than a given value;\n",
    " * to simulate stanzas, an empty line is added after verses ending with a full stop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1332bf",
   "metadata": {},
   "source": [
    "### Examples - Generating the First Verse\n",
    "\n",
    "Sequence to sequence model, training examples are built by taking poetry lines and sampling words from the line. \n",
    "\n",
    "The extracted words are used as source (mimic keywords), the poem line from which the words are extracted is used as target.\n",
    "\n",
    "At inference time, the model returns a poem line given some input keywords provided by the user.\n",
    "\n",
    "mBART model fine-tuned with poetry lines from Gutenberg project and Wikisource (@Sardana can comment on this).\n",
    "\n",
    "\n",
    "#### Some examples with 2 random keywords\n",
    "\n",
    "**amerikkalainen onnettomuus**\n",
    " * Onnettomuus on minussa, minä amerikkalainen,\n",
    " * Ja onnettomuus rasittaa, – minä olen amerikkalainen,\n",
    "\n",
    "**eläin uudistus**\n",
    " * Eläin uudistus. Maa vapisi, puu laski,\n",
    " * Eläin uudistus. Maa ja taivas, maa\n",
    " * Eläin uudistus. Maa vapisi, puu lakastui,\n",
    " * Eläin uudistus. Maa vapisi, puu katosi,\n",
    " * Eläin uudistus! tuo ihmeinen eläin, tuo\n",
    "\n",
    "**uudistus ase**\n",
    " * Tule, ase uudistus! Tulkoon valkeus tuo!\n",
    "\n",
    "**ase korvaus**\n",
    " * Korvaus, ase oikea on.\n",
    " * Se on ase, millä korvaus\n",
    " * Ase, korvaus kalliin hinnan!\n",
    " * Ase, korvaus kalliin miehen ja vaivan,\n",
    " * Se on ase, jonka korvaus\n",
    "\n",
    "**korvaus pelkästään**\n",
    " * On korvaus pelkästään hurskas.\n",
    " * On korvaus pelkästään unelmien hullun.\n",
    "\n",
    "**pelkästään päätyä**\n",
    " * Ei ole meidän päätyä pelkästään.\n",
    " * Se elämän on pelkästään päätyä.\n",
    "\n",
    "**päätyä heikko**\n",
    " * Pian alkoi päivä heikko päätyä,\n",
    " * Pian alkoi sen matka heikko päätyä.\n",
    "\n",
    "**heikko säilyttää**\n",
    " * Säilyttää, mi heikko on vaan.\n",
    "\n",
    "**säilyttää kyetä**:\n",
    " * Kyetä ei, vaan säilyttää aatetta\n",
    " * Kyetä ei, jot'säilyttää vain haluaa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a2b772",
   "metadata": {},
   "source": [
    "### Examples - Next-Line Iterative Generation\n",
    " \n",
    "Sequence to sequence model (fine-tuned mBART), training data is build by concatenating lines from poems scraped from finnish wikisource.\n",
    "\n",
    "First sentence from previous model, with keywords _amerikkalainen, onnettomuus_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cd00672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Se puuhaa ja puuhaa ja puuhaa,\n",
      "2) Ja koiran tiede koiran tekee.\n",
      "3) Kun tiede koira on : se puuhaa,\n",
      "4) Ja tutkii ja tekee työtä.\n"
     ]
    }
   ],
   "source": [
    "input_line = \"Ja tiede koira on : se etsii,\"\n",
    "\n",
    "candidates = list(set([c for c in get_next_line_candidates(input_line) if remove_punct(c)]))\n",
    "\n",
    "for i, candidate in enumerate(candidates):\n",
    "    print(\"{}) {}\".format(i + 1, candidate))\n",
    "\n",
    "out = input_line\n",
    "next_candidates = candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10af8d86",
   "metadata": {},
   "source": [
    "This cell can be used multiple times to generate additional lines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69a2fbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial poem:\n",
      "#############\n",
      "\n",
      "Ja tiede koira on : se etsii,\n",
      "Ja tutkii ja tekee työtä.\n",
      "Oi, Herra, anna armosta\n",
      "\n",
      "Next-Line Candidates:\n",
      "#####################\n",
      "\n",
      "1) Sun siunauksens’ aina,\n",
      "2) Ja voimaa, tarmoa meissä,\n",
      "3) Ja anteeks’ syntimme,\n",
      "4) Sun siunattu pelastus!\n"
     ]
    }
   ],
   "source": [
    "n = 2\n",
    "\n",
    "next_input_line = next_candidates[n-1]\n",
    "out += \"\\n\" + next_input_line\n",
    "\n",
    "print(\"Partial poem:\\n#############\\n\\n{}\\n\".format(out))\n",
    "\n",
    "next_candidates = list(set([c for c in get_next_line_candidates(next_input_line) if remove_punct(c)]))\n",
    "\n",
    "print(\"Next-Line Candidates:\\n#####################\\n\")\n",
    "for i, candidate in enumerate(next_candidates):\n",
    "    print(\"{}) {}\".format(i + 1, candidate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc843762",
   "metadata": {},
   "source": [
    "### Example - Unsupervised Iterative Generation\n",
    "\n",
    "Used for prototyping, was useful to observe looping behaviour (partially solved by increasing the number of epochs for the next-line generative model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8098495e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/micheleb/miniconda3/envs/finnishPoetryGeneration/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      "INFO:root:Generated candidates ['se puuhaa, tutkii ja puuhaa.', 'Ja tutkii ja tekee työtä.', 'Se tutkii ja tekee työtä;', 'Se tutkii ja tekee työtä;', 'Ja tutkii ja tekee työtä.']\n",
      "INFO:root:Generated poetry line 'se puuhaa, tutkii ja puuhaa.'\n",
      "INFO:root:Generated candidates ['Oi ihminen, niin monta on murhaa,', 'Ja aina, aina se puuhaa vaan,', 'Niin kauan kuin Suomessa yksikään', 'Se tietää – mut ei tiedä – kumpikaan,', 'Se tietää, ett’ on ihminen se,']\n",
      "INFO:root:Generated poetry line 'Oi ihminen, niin monta on murhaa,'\n",
      "INFO:root:Generated candidates ['ei yksi murhaa murhaa.', 'on murhaa, vainoa ja riettautta.', 'on murhaa, vainoa, vainoa.', 'on murhaa, vainoa, vainoa.', 'ei murhaa, vaan taistelua, taistelua.']\n",
      "INFO:root:Generated poetry line 'ei yksi murhaa murhaa.'\n",
      "INFO:root:Generated candidates ['– – – – –', 'Ei yksikään murhaa murhaa,', 'Ei yksikään saa surra,', 'Ei murhaa murhaa, ei!', 'Ei estä yksikään, ei!']\n",
      "INFO:root:Generated poetry line 'Ei estä yksikään, ei!'\n",
      "INFO:root:Generated candidates ['Ei estä yksikään, ei!', 'Ei eestä yksikään, ei!', 'Vaan vapauden päivä koittaa,', 'Ei estä yksikään, ei!', 'Ei estä yksikään, ei!']\n",
      "INFO:root:Generated poetry line 'Vaan vapauden päivä koittaa,'\n",
      "INFO:root:Generated candidates ['Se koittaa vapauden yöstä!', 'Ja oikeuden aamu koittaa,', 'Se koittaa vapauden yöstä!', 'Ja oikeuden aamu koittaa,', 'Ja oikeuden aamu koittaa,']\n",
      "INFO:root:Generated poetry line 'Ja oikeuden aamu koittaa,'\n",
      "INFO:root:Generated candidates ['Ja vapauden päivä koittaa,', 'Ja oikeus, jonk’ oikeus voittaa,', 'Ja vapauden päivä koittaa,', 'Ja oikeutta kaikille se julistaa,', 'Ja vapauden päivä koittaa,']\n",
      "INFO:root:Generated poetry line 'Ja oikeutta kaikille se julistaa,'\n",
      "INFO:root:Generated candidates ['Ja vapauden kieltä se lausuu.', 'Ja vapauden lipun kankahalle kantaa,', 'Ja vapauden lipun kankahalle kantaa,', 'Ja vapautta myöskin se julistaa.', 'Ja vapauden kieltä kirkastaa.']\n",
      "INFO:root:Generated poetry line 'Ja vapauden kieltä se lausuu.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ja tiede koira on : se etsii,\n",
      "se puuhaa, tutkii ja puuhaa.\n",
      "Oi ihminen, niin monta on murhaa,\n",
      "ei yksi murhaa murhaa.\n",
      "\n",
      "Ei estä yksikään, ei!\n",
      "Vaan vapauden päivä koittaa,\n",
      "Ja oikeuden aamu koittaa,\n",
      "Ja oikeutta kaikille se julistaa,\n",
      "Ja vapauden kieltä se lausuu.\n"
     ]
    }
   ],
   "source": [
    "input_line = \"Ja tiede koira on : se etsii,\"\n",
    "\n",
    "poem = iterative_generation(input_line, 5)\n",
    "print(poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d347dce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
